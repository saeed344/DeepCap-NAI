{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82978,
     "status": "ok",
     "timestamp": 1756111327921,
     "user": {
      "displayName": "Saeed Ahmed",
      "userId": "16724720564507897686"
     },
     "user_tz": -300
    },
    "id": "Xkxq030b8pPP",
    "outputId": "ddbbbda6-9ee3-4fbd-bb60-dc85a7beb63c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12568,
     "status": "ok",
     "timestamp": 1730262287399,
     "user": {
      "displayName": "Saeed Ahmed",
      "userId": "16724720564507897686"
     },
     "user_tz": -300
    },
    "id": "MIrCoFXckIdQ",
    "outputId": "fe96b154-4ffb-476c-a20a-0208dbac99a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step\n",
      "Fold 1: acc=0.936416, sensitivity=0.957447, specificity=0.911392, mcc=0.872016, f1=0.942408, roc_auc=0.966402, pr_auc=0.960335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step\n",
      "Fold 2: acc=0.959538, sensitivity=0.946809, specificity=0.974684, mcc=0.919267, f1=0.962162, roc_auc=0.971115, pr_auc=0.960232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step\n",
      "Fold 3: acc=0.965318, sensitivity=0.978723, specificity=0.949367, mcc=0.930222, f1=0.968421, roc_auc=0.975761, pr_auc=0.971071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step\n",
      "Fold 4: acc=0.942197, sensitivity=0.936170, specificity=0.949367, mcc=0.883991, f1=0.946237, roc_auc=0.973270, pr_auc=0.962334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step\n",
      "Fold 5: acc=0.971098, sensitivity=0.978723, specificity=0.962025, mcc=0.941764, f1=0.973545, roc_auc=0.976569, pr_auc=0.971659\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Make sure to import TensorFlow for the squash function\n",
    "\n",
    "# Define the path and folder name\n",
    "path = \"/content/drive/MyDrive/Watashara_Projects/NIA/\"\n",
    "\n",
    "# Load your test data\n",
    "Xt = X_ind  # Ensure X_ind is defined in your environment\n",
    "yt = y_ind  # Ensure y_ind is defined in your environment\n",
    "\n",
    "# Reshape Xt to the appropriate shape for the model\n",
    "[sample_num, input_dim] = np.shape(Xt)\n",
    "Xt = Xt.reshape(-1, 1, input_dim, 1)  # Assign the reshaped array back to Xt\n",
    "\n",
    "sepscores = []\n",
    "ytest = np.ones((1, 2)) * 0.5\n",
    "yscore = np.ones((1, 2)) * 0.5\n",
    "\n",
    "for i in range(5):\n",
    "    loaded_model = load_model(\n",
    "        path + f'Results/{fld_name}/capsule_network_model_fold_{i + 1}.h5',\n",
    "        custom_objects={'squash': squash}  # Register the custom function here\n",
    "    )\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    # Evaluate loaded model on test data\n",
    "    y_score = loaded_model.predict(Xt)\n",
    "    y_class = categorical_probas_to_classes(y_score)\n",
    "\n",
    "    y_test = to_categorical(yt)\n",
    "    ytest = np.vstack((ytest, y_test))\n",
    "    yscore = np.vstack((yscore, y_score))\n",
    "\n",
    "    acc, precision, npv, sensitivity, specificity, mcc, f1 = calculate_performance(len(y_class), y_class, yt)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test[:, 1], y_score[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)  # Calculate ROC AUC\n",
    "\n",
    "    # Calculate AUPR\n",
    "    aupr = average_precision_score(y_test[:, 1], y_score[:, 1])\n",
    "\n",
    "    # Calculate Precision-Recall AUC\n",
    "    precision_vals, recall, _ = precision_recall_curve(y_test[:, 1], y_score[:, 1])\n",
    "    pr_auc = auc(recall, precision_vals)\n",
    "\n",
    "    sepscores.append([acc, sensitivity, specificity, mcc, f1, roc_auc, pr_auc])\n",
    "    print(f'Fold {i + 1}: acc={acc:.6f}, sensitivity={sensitivity:.6f}, specificity={specificity:.6f}, '\n",
    "          f'mcc={mcc:.6f}, f1={f1:.6f}, roc_auc={roc_auc:.6f}, pr_auc={pr_auc:.6f}')\n",
    "\n",
    "# Prepare results for saving\n",
    "row = ytest.shape[0]\n",
    "ytest = ytest[np.array(range(1, row)), :]\n",
    "ytest_sum = pd.DataFrame(data=ytest)\n",
    "ytest_sum.to_csv(path + f'Results/{fld_name}/Caps_ytest_test.csv')\n",
    "\n",
    "yscore_ = yscore[np.array(range(1, row)), :]\n",
    "yscore_sum = pd.DataFrame(data=yscore_)\n",
    "yscore_sum.to_csv(path + f'Results/{fld_name}/Caps_yscore_test.csv')\n",
    "\n",
    "# Calculate mean results\n",
    "scores = np.array(sepscores)\n",
    "result1 = np.mean(scores, axis=0)\n",
    "H1 = result1.tolist()\n",
    "sepscores.append(H1)\n",
    "result = sepscores\n",
    "\n",
    "data_csv = pd.DataFrame(data=result)\n",
    "data_csv.to_csv(path + f'Results/{fld_name}/Caps_allfeat_test_results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN7ZPs1w5Bmobh18wI18mDk",
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
