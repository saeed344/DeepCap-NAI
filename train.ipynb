{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82978,
     "status": "ok",
     "timestamp": 1756111327921,
     "user": {
      "displayName": "Saeed Ahmed",
      "userId": "16724720564507897686"
     },
     "user_tz": -300
    },
    "id": "Xkxq030b8pPP",
    "outputId": "ddbbbda6-9ee3-4fbd-bb60-dc85a7beb63c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "OapNSmY8ugv2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from keras import layers, models\n",
    "from sklearn.preprocessing import scale\n",
    "import keras.backend as K\n",
    "from keras import layers, models\n",
    "from keras.layers import Input, Dense, Layer, Reshape, Flatten, BatchNormalization, Dropout, Lambda, Activation, Multiply, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "path = \"/content/drive/MyDrive/Watashara_Projects/NIA/\"\n",
    "\n",
    "\n",
    "shapewx = [1, input_dimwx, 1]\n",
    "labels = pd.read_csv(path + 'Features/AP2D.csv')\n",
    "labels['Activity'] = labels['Class'].map({'Active': 1, 'Inactive': 0})\n",
    "label = labels['Activity'].values\n",
    "\n",
    "def get_shuffle(data, label):\n",
    "    index = np.arange(len(label))\n",
    "    np.random.shuffle(index)\n",
    "    return data[index], label[index]\n",
    "\n",
    "(datasetwx, label) = get_shuffle(datasetwx, label)\n",
    "\n",
    "def scale_mean_var(input_arr, axis=0):\n",
    "    mean_ = np.mean(input_arr, axis=0)\n",
    "    scale_ = np.std(input_arr, axis=0)\n",
    "    output_arr = input_arr - mean_\n",
    "    scale_[scale_ == 0.0] = 1.0\n",
    "    output_arr /= scale_\n",
    "    return output_arr\n",
    "\n",
    "# Define Capsule Network\n",
    "def squash(vectors, axis=-1):\n",
    "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "def build_capsule_network(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional layer\n",
    "    x = layers.Conv2D(64, (1, 9), strides=2, padding='valid', name='conv1')(input_layer)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "    # Primary Capsule Layer\n",
    "    x = layers.Conv2D(32, (1, 3), strides=2, padding='valid', name='primarycap_conv2')(x)\n",
    "    [_, _, num_caps, _] = x.shape\n",
    "    num_caps = int(num_caps)\n",
    "    x = Reshape(target_shape=[-1, num_caps], name='primarycap_reshape')(x)\n",
    "\n",
    "    # Apply squash function\n",
    "    x = Lambda(squash, output_shape=(x.shape[1], num_caps), name='primarycap_squash')(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "    # Digit Capsule Layer with routing mechanism\n",
    "    x = Flatten()(x)\n",
    "    uhat = Dense(128, kernel_initializer='he_normal', bias_initializer='zeros', name='uhat_digitcaps')(x)\n",
    "    c = Activation('softmax', name='softmax_digitcaps1')(uhat)\n",
    "    c = Dense(128)(c)\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "\n",
    "    # Repeat routing process\n",
    "    for _ in range(3):\n",
    "        c = Activation('softmax')(s_j)\n",
    "        c = Dense(128)(c)\n",
    "        x = Multiply()([uhat, c])\n",
    "        s_j = LeakyReLU()(x)\n",
    "\n",
    "    # Output layer with Dropout\n",
    "    s_j = Reshape((-1, 128, 1))(s_j)\n",
    "    cbam = layers.GlobalAveragePooling2D()(s_j)\n",
    "    cbam = Dropout(0.5)(cbam)  # Added Dropout for regularization\n",
    "    cbam = Dense(2, activation='sigmoid')(cbam)\n",
    "\n",
    "    return Model(input_layer, cbam)\n",
    "\n",
    "# Train and evaluate the Capsule Network\n",
    "def categorical_probas_to_classes(p):\n",
    "    return np.argmax(p, axis=1)\n",
    "\n",
    "\n",
    "ytest = np.ones((1, 2)) * 0.5\n",
    "yscore = np.ones((1, 2)) * 0.5\n",
    "sepscores = []\n",
    "\n",
    "# Build and compile the Capsule Network\n",
    "capsule_network = build_capsule_network((1, input_dimwx, 1))\n",
    "capsule_network.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "if not os.path.exists(path + (f'Results/{fld_name}/')):\n",
    "    os.makedirs(path + f'Results/{fld_name}/')\n",
    "\n",
    "X_train, X_ind, y_train, y_ind = train_test_split(datasetwx, label, test_size=0.2, random_state=42)\n",
    "\n",
    "for fold_index, (train, test) in enumerate(skf.split(X_train, y_train)):\n",
    "    label_train = to_categorical(label[train])\n",
    "    X = datasetwx\n",
    "\n",
    "    # Reshape the input data\n",
    "    X_train_fold = X[train].reshape(-1, 1, input_dimwx, 1)\n",
    "    X_test_fold = X[test].reshape(-1, 1, input_dimwx, 1)\n",
    "\n",
    "    # Initialize a new model for each fold\n",
    "    capsule_network_fold = build_capsule_network((1, input_dimwx, 1))\n",
    "    capsule_network_fold.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])#0.0002, 0.5\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Learning rate adjustment\n",
    "    lr_reduction = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    history = capsule_network_fold.fit(\n",
    "        X_train_fold,\n",
    "        label_train,\n",
    "        epochs=30,\n",
    "        validation_data=(X_test_fold, to_categorical(label[test])),\n",
    "        callbacks=[early_stopping, lr_reduction]\n",
    "    )\n",
    "\n",
    "    # Save the model after training on each fold with a unique filename\n",
    "    model_save_path = path + f'Results/{fld_name}/capsule_network_model_fold_{fold_index + 1}.h5'\n",
    "    capsule_network_fold.save(model_save_path)\n",
    "\n",
    "    y_test = to_categorical(label[test])\n",
    "    ytest = np.vstack((ytest, y_test))\n",
    "    y_score = capsule_network_fold.predict(X_test_fold)\n",
    "    yscore = np.vstack((yscore, y_score))\n",
    "    fpr, tpr, _ = roc_curve(y_test[:, 0], y_score[:, 0])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    y_class = categorical_probas_to_classes(y_score)\n",
    "    acc, precision, npv, sensitivity, specificity, mcc, f1 = calculate_performance(len(y_class), y_class, label[test])\n",
    "    sepscores.append([acc, precision, npv, sensitivity, specificity, mcc, f1, roc_auc])\n",
    "\n",
    "    print(f'Fold {fold_index + 1}: acc={acc},  sensitivity={sensitivity}, specificity={specificity}, mcc={mcc}, f1={f1}, roc_auc={roc_auc}')\n",
    "\n",
    "scores = np.array(sepscores)\n",
    "\n",
    "result1 = np.mean(scores, axis=0)\n",
    "H1 = result1.tolist()\n",
    "sepscores.append(H1)\n",
    "result = sepscores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12568,
     "status": "ok",
     "timestamp": 1730262287399,
     "user": {
      "displayName": "Saeed Ahmed",
      "userId": "16724720564507897686"
     },
     "user_tz": -300
    },
    "id": "MIrCoFXckIdQ",
    "outputId": "fe96b154-4ffb-476c-a20a-0208dbac99a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step\n",
      "Fold 1: acc=0.936416, sensitivity=0.957447, specificity=0.911392, mcc=0.872016, f1=0.942408, roc_auc=0.966402, pr_auc=0.960335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step\n",
      "Fold 2: acc=0.959538, sensitivity=0.946809, specificity=0.974684, mcc=0.919267, f1=0.962162, roc_auc=0.971115, pr_auc=0.960232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step\n",
      "Fold 3: acc=0.965318, sensitivity=0.978723, specificity=0.949367, mcc=0.930222, f1=0.968421, roc_auc=0.975761, pr_auc=0.971071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step\n",
      "Fold 4: acc=0.942197, sensitivity=0.936170, specificity=0.949367, mcc=0.883991, f1=0.946237, roc_auc=0.973270, pr_auc=0.962334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step\n",
      "Fold 5: acc=0.971098, sensitivity=0.978723, specificity=0.962025, mcc=0.941764, f1=0.973545, roc_auc=0.976569, pr_auc=0.971659\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Make sure to import TensorFlow for the squash function\n",
    "\n",
    "# Define the path and folder name\n",
    "path = \"/content/drive/MyDrive/Watashara_Projects/NIA/\"\n",
    "# fld_name = 'Caps_all_feat'\n",
    "\n",
    "# Define your custom squash function\n",
    "def squash(x):\n",
    "    norm = tf.norm(x, axis=-1, keepdims=True)\n",
    "    squared_norm = tf.square(norm)\n",
    "    scale = squared_norm / (1 + squared_norm) / norm\n",
    "    return scale * x\n",
    "\n",
    "# Load your test data\n",
    "Xt = X_ind  # Ensure X_ind is defined in your environment\n",
    "yt = y_ind  # Ensure y_ind is defined in your environment\n",
    "\n",
    "# Reshape Xt to the appropriate shape for the model\n",
    "[sample_num, input_dim] = np.shape(Xt)\n",
    "Xt = Xt.reshape(-1, 1, input_dim, 1)  # Assign the reshaped array back to Xt\n",
    "\n",
    "sepscores = []\n",
    "ytest = np.ones((1, 2)) * 0.5\n",
    "yscore = np.ones((1, 2)) * 0.5\n",
    "\n",
    "for i in range(5):\n",
    "    loaded_model = load_model(\n",
    "        path + f'Results/{fld_name}/capsule_network_model_fold_{i + 1}.h5',\n",
    "        custom_objects={'squash': squash}  # Register the custom function here\n",
    "    )\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    # Evaluate loaded model on test data\n",
    "    y_score = loaded_model.predict(Xt)\n",
    "    y_class = categorical_probas_to_classes(y_score)\n",
    "\n",
    "    y_test = to_categorical(yt)\n",
    "    ytest = np.vstack((ytest, y_test))\n",
    "    yscore = np.vstack((yscore, y_score))\n",
    "\n",
    "    acc, precision, npv, sensitivity, specificity, mcc, f1 = calculate_performance(len(y_class), y_class, yt)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test[:, 1], y_score[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)  # Calculate ROC AUC\n",
    "\n",
    "    # Calculate AUPR\n",
    "    aupr = average_precision_score(y_test[:, 1], y_score[:, 1])\n",
    "\n",
    "    # Calculate Precision-Recall AUC\n",
    "    precision_vals, recall, _ = precision_recall_curve(y_test[:, 1], y_score[:, 1])\n",
    "    pr_auc = auc(recall, precision_vals)\n",
    "\n",
    "    sepscores.append([acc, sensitivity, specificity, mcc, f1, roc_auc, pr_auc])\n",
    "    print(f'Fold {i + 1}: acc={acc:.6f}, sensitivity={sensitivity:.6f}, specificity={specificity:.6f}, '\n",
    "          f'mcc={mcc:.6f}, f1={f1:.6f}, roc_auc={roc_auc:.6f}, pr_auc={pr_auc:.6f}')\n",
    "\n",
    "# Prepare results for saving\n",
    "row = ytest.shape[0]\n",
    "ytest = ytest[np.array(range(1, row)), :]\n",
    "ytest_sum = pd.DataFrame(data=ytest)\n",
    "ytest_sum.to_csv(path + f'Results/{fld_name}/Caps_ytest_test.csv')\n",
    "\n",
    "yscore_ = yscore[np.array(range(1, row)), :]\n",
    "yscore_sum = pd.DataFrame(data=yscore_)\n",
    "yscore_sum.to_csv(path + f'Results/{fld_name}/Caps_yscore_test.csv')\n",
    "\n",
    "# Calculate mean results\n",
    "scores = np.array(sepscores)\n",
    "result1 = np.mean(scores, axis=0)\n",
    "H1 = result1.tolist()\n",
    "sepscores.append(H1)\n",
    "result = sepscores\n",
    "\n",
    "data_csv = pd.DataFrame(data=result)\n",
    "data_csv.to_csv(path + f'Results/{fld_name}/Caps_allfeat_test_results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN7ZPs1w5Bmobh18wI18mDk",
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
